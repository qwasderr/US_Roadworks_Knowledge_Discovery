#%%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, mean_absolute_error, mean_squared_error
from xgboost import XGBClassifier, XGBRegressor
from sklearn.cluster import KMeans
from mlxtend.frequent_patterns import apriori, association_rules
import logging
from joblib import Parallel, delayed

pd.set_option('display.max_colwidth', None)  # –ù–µ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –¥–æ–≤–≥—ñ –∫–æ–ª–æ–Ω–∫–∏
pd.set_option('display.expand_frame_repr', False)  # –í—ñ–¥–∫–ª—é—á–∞—î –∞–≤—Ç–æ-–ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–Ω—è

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
logging.info("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö...")
df = pd.read_csv("~/Downloads/construction_data2.csv")

# –í–∏–¥–∞–ª–µ–Ω–Ω—è —Ä—è–¥–∫—ñ–≤ –∑ –ø–æ—Ä–æ–∂–Ω—ñ–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
logging.info("–í–∏–¥–∞–ª–µ–Ω–Ω—è —Ä—è–¥–∫—ñ–≤ –∑ –ø–æ—Ä–æ–∂–Ω—ñ–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏...")
df.dropna(inplace=True)

# –í–∏–±—ñ—Ä 10% –¥–∞–Ω–∏—Ö –¥–ª—è –ø–æ–¥–∞–ª—å—à–æ—ó –æ–±—Ä–æ–±–∫–∏
logging.info("–í–∏–±—ñ—Ä 10% –¥–∞–Ω–∏—Ö –¥–ª—è –æ–±—Ä–æ–±–∫–∏...")
df = df.sample(frac=0.1, random_state=42)

# –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è —á–∞—Å—É
logging.info("–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è —á–∞—Å—É...")
df["Start_Time"] = pd.to_datetime(df["Start_Time"], format='mixed', errors='coerce')
df["End_Time"] = pd.to_datetime(df["End_Time"], format='mixed', errors='coerce')
df["Duration"] = (df["End_Time"] - df["Start_Time"]).dt.total_seconds() / 3600  # —É –≥–æ–¥–∏–Ω–∞—Ö
# –ó–∞–∫–æ–¥—É—î–º–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ –∑–º—ñ–Ω–Ω—ñ
logging.info("–ó–∞–∫–æ–¥—É–≤–∞–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö...")
label_encoders = {}
for col in df.select_dtypes(include="object").columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le
#%%
# –ó–∞–≤–¥–∞–Ω–Ω—è 1: –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è —Ä—ñ–≤–Ω—è –≤–ø–ª–∏–≤—É (Severity)
logging.info("–ü–æ—á–∞—Ç–æ–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è —Ä—ñ–≤–Ω—è –≤–ø–ª–∏–≤—É...")
X = df.drop(columns=["Severity", "ID", "Start_Time", "End_Time"])
y = df["Severity"]
y = y - 1
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –¥–ª—è –∫—Ä–∞—â–æ—ó –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü—ñ—ó
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),  # Increased max_iter
    "Random Forest": RandomForestClassifier(n_estimators=100, n_jobs=-1),  # Use all cores
    "XGBoost": XGBClassifier(eval_metric="mlogloss", n_jobs=-1)
}

def train_and_evaluate(name, model):
    logging.info(f"–¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {name}...")
    model.fit(X_train_scaled, y_train)  # Fit with scaled data
    y_pred = model.predict(X_test_scaled)  # Predict with scaled data
    print(y_pred)
    result = {
        'model': name,
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred, average='weighted', zero_division=1),
        'recall': recall_score(y_test, y_pred, average='weighted', zero_division=1),
        'f1': f1_score(y_test, y_pred, average='weighted', zero_division=1),
        "y_pred": y_pred
    }
    return result

# Parallelize model training
results = Parallel(n_jobs=-1)(delayed(train_and_evaluate)(name, model) for name, model in models.items())
# Print results
for result in results:
    print(f"  Model: {result['model']}")
    print(f"  Accuracy: {result['accuracy']:.4f}")
    print(f"  Precision: {result['precision']:.4f}")
    print(f"  Recall: {result['recall']:.4f}")
    print(f"  F1-score: {result['f1']:.4f}")
    print(result['y_pred'])
    print()
#%%
# –ó–∞–≤–¥–∞–Ω–Ω—è 2: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è –¥–æ—Ä–æ–∂–Ω—ñ—Ö —Ä–æ–±—ñ—Ç –∑ —ñ–Ω—à–∏–º–∏ –æ–∑–Ω–∞–∫–∞–º–∏ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é K-Means
logging.info("–ü–æ—á–∞—Ç–æ–∫ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó –¥–æ—Ä–æ–∂–Ω—ñ—Ö —Ä–æ–±—ñ—Ç –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é K-Means –∑ –æ–∑–Ω–∞–∫–∞–º–∏ 'Distance(mi)', 'Temperature(F)', 'Wind_Speed(mph)'...")

# –í–∏–±—ñ—Ä –æ–∑–Ω–∞–∫ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó –±–µ–∑ Severity
X_clustering = df[["Distance(mi)", "Temperature(F)", "Wind_Speed(mph)"]]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_clustering)

# K-Means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è
kmeans = KMeans(n_clusters=4, random_state=42)
df["Cluster_KMeans"] = kmeans.fit_predict(X_scaled)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
logging.info("–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ K-Means...")
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=df["Cluster_KMeans"], palette="viridis", alpha=0.5)
plt.title("K-Means Clustering with Distance, Temperature, and Wind Speed")
plt.xlabel("Feature 1 (Distance)")
plt.ylabel("Feature 2 (Temperature)")
plt.legend(title="Cluster")
plt.show()
#%%
# –ó–∞–≤–¥–∞–Ω–Ω—è 3: –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è —Ç—Ä–∏–≤–∞–ª–æ—Å—Ç—ñ –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞
logging.info("–ü–æ—á–∞—Ç–æ–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è —Ç—Ä–∏–≤–∞–ª–æ—Å—Ç—ñ –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞...")
X = df.drop(columns=["Duration", "ID", "Start_Time", "End_Time"])
y = df["Duration"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

models = {
    "Linear Regression": LinearRegression(),
    "Random Forest Regressor": RandomForestRegressor(n_estimators=100, n_jobs=-1),  # Use all cores
    "XGBoost Regressor": XGBRegressor(n_jobs=-1)
}

# Define the train and evaluate function for regression
def train_and_evaluate_regressor(name, model):
    logging.info(f"–¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {name}...")
    model.fit(X_train, y_train)  # Fit the model on training data
    y_pred = model.predict(X_test)  # Predict on test data
    result = {
        'model': name,
        'MAE': mean_absolute_error(y_test, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),
        "y_test": y_test,
        "y_pred": y_pred
    }
    return result

# Parallelize model training
results = Parallel(n_jobs=-1)(delayed(train_and_evaluate_regressor)(name, model) for name, model in models.items())

# Print the results
for result in results:
    print(f"\n{result['model']}:")
    print(f"  MAE: {result['MAE']:.4f}")
    print(f"  RMSE: {result['RMSE']:.4f}")
    print(result["y_test"][:5])
    print(result['y_pred'][:5])


#%%
logging.info("–ü–æ—á–∞—Ç–æ–∫ –≤–∏—è–≤–ª–µ–Ω–Ω—è –∞—Å–æ—Ü—ñ–∞—Ç–∏–≤–Ω–∏—Ö –ø—Ä–∞–≤–∏–ª...")


# –í–∏–±–∏—Ä–∞—î–º–æ –±—ñ–ª—å—à–µ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
features = ["Severity", "Distance(mi)", "Temperature(F)", "Humidity(%)",
            "Precipitation(in)", "Wind_Speed(mph)", "Visibility(mi)"]
df_bin = df[features].copy()

# –ë—ñ–Ω–∞—Ä–∏–∑–∞—Ü—ñ—è —á–∏—Å–ª–æ–≤–∏—Ö –∑–Ω–∞—á–µ–Ω—å (—Ä–æ–∑–±–∏—Ç—Ç—è –Ω–∞ 5 –∫–∞—Ç–µ–≥–æ—Ä—ñ–π)
for col in df_bin.columns:
    df_bin[col] = pd.qcut(df_bin[col], q=5, labels=False, duplicates='drop')

# –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è —É –±—É–ª–µ–≤–∏–π —Ñ–æ—Ä–º–∞—Ç
df_bin = df_bin.astype("bool")

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∞–ª–≥–æ—Ä–∏—Ç–º—É Apriori
frequent_itemsets = apriori(df_bin, min_support=0.02, use_colnames=True)
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.8)

# –í–∏–≤–µ–¥–µ–Ω–Ω—è –∑–Ω–∞–π–¥–µ–Ω–∏—Ö –∑–∞–∫–æ–Ω–æ–º—ñ—Ä–Ω–æ—Å—Ç–µ–π
print("\nüîπ –í–∏—è–≤–ª–µ–Ω—ñ –∞—Å–æ—Ü—ñ–∞—Ç–∏–≤–Ω—ñ –ø—Ä–∞–≤–∏–ª–∞:")
print(rules[["antecedents", "consequents", "support", "confidence", "lift"]])